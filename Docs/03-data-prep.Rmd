# Inferential Models

## Intuition: Why inferential models?

Inferential models are essential to understanding our data.  You may be wondering,
why not visualize the data first with plots?  In my opinion, visualizations can 
be misleading due to correlations between predictors. We'll explore these concept in detail.

### Correlations

To start with a trivial example, let's pick two predictors that are obviously
correlated: attained age and duration.  Using the Perm dataframe we built in the
preceding section, the correlation coefficient is `r round(cor(df_perm$Duration, df_perm$Attained_Age), 2)`. 
Although they are correlated, there are two distinct actuarial considerations here: the selection
period wear-off, and the age-related mortality effect, defined as follows:

* **Selection Period Wear-off:** When you buy individual life insurance, there
is usually some form of underwriting.  It may be as simple as a questionnaire about
your health, or as involved as blood-tests or an exam by a physician. The insurer
has some assurances that the insured has some level of health, but as time goes on,
the relevance of the health assessment at duration 1 diminishes. For this reason, 
the expected mortality rate of a 55 year old 10 years after issue is not the same as 
a 55 year old at issue.

* **Age-related mortality effect:** It's no surprise that as we get older, there
is an increased mortality rate.  Our bodies wear-down over time, so this effect
is another major consideration 

### Inferential models

The risk of mis-attributing a rise in the mortality rate to a correlated predictor, 
is lowered when we allow a model to help us decide which predictor is best.  
We still need to apply good judgement, and our knowledge of the context of the data
in deciding if we believe the model.


## Technical Tools: Decision Trees

Decision trees are very easy to interpret, making them an excellent tool for 
communication.  Although we'll look at the outputs of the trees, it's wise
to manually convert the output to tabular form if you are communicating your
findings to stakeholders.

We built two data frames earlier in this section, one for perm business, the
other of ULSG (Universal Life with Secondary Guarantees).  We discussed our
reasons for treating this business separately above.

Before applying a decision tree to the Perm dataframe, let's see how our
expected basis lines up with the actual number of deaths.  We are using the 
2008 VBT because the 2017 VBT is actually built on this data.

### Actual-to-Expected Plot

To get a sense of the fit the provided by the expected basis, we'll examine
both the overall A/E ratio and the ratio at each age and gender.  One problem
with using the overall A/E ratio alone is that errors can cancel out.  If one
part of the expected curve is over-predicting deaths, and the other is under-predicting,
you can end up with ratio close to 100%.  Its important to examine both the ratio
and the curve to ensure you are not misled.  We see some of this canceling out of 
errors in the below plot, note that the curve dips under 90% for between ages 50 
and 70, but the overall A/E of 93% doesn't convey the extent of the misfit.

```{r}

ae_plot_data <- df_perm %>%
  group_by(Attained_Age, Gender) %>%
  filter(Expected_Death_QX2008VBT_by_Policy > 0) %>%
  summarise(
    `Total Deaths` = sum(Number_Of_Deaths),
    `Expected (08 VBT)` = sum(Expected_Death_QX2008VBT_by_Policy),
    .groups = "drop"
  ) %>%
  filter(
    `Total Deaths` >= 30
  )

ggplot(ae_plot_data, aes(x = Attained_Age, y=`Total Deaths` / `Expected (08 VBT)`,
                    color=Gender)) +
  geom_smooth(se=F) +
  scale_y_continuous(labels = scales::percent_format(),
                     transform = scales::pseudo_log_trans()) +
  geom_hline(yintercept = 1) +
  theme_bw() +
  ylab("Actual / Expected") +
  xlab("Attained Age") +
  ggtitle(sprintf(
    "Actual-to-Expected Ratio: %d%%",
    (round(100*
      sum(ae_plot_data$`Total Deaths`) / sum(ae_plot_data$`Expected (08 VBT)`),0))), 
    "Perm Business: By Age & Gender")

```

### Decision Tree

We've confirmed that the fit can be better using the above graph, but where is 
the best place to start?  Step-wise regression is one option, but a decision tree
will likely give you the same answer for the first predictor, with the added benefit 
of immediate interpretability.  Unlike step-wise regression, a decision tree can
give us some insights about *interactions*, where two predictors need to be considered
in tandem.

```{r}

if (!require("rpart")) {
  install.packages("rpart")
  library(rpart)
}

if (!require("rpart.plot")) {
  install.packages("rpart.plot")
  library(rpart.plot)
}

prep_rpart_data <- function(df) {
  df %>% 
    filter(Expected_Death_QX2008VBT_by_Policy > 0) %>%
    select(-Amount_Exposed, 
           -Death_Claim_Amount, 
           -Policies_Exposed,
           -Age_Basis) 
}

remove_response_vars <- function(df) {
  df %>%
    select(-Number_Of_Deaths, 
           -Expected_Death_QX2008VBT_by_Policy)
}

df_rpart <- df_perm %>%
  prep_rpart_data()

# build the decision tree
rpart_perm <- rpart(
  as.matrix(df_rpart[, c("Expected_Death_QX2008VBT_by_Policy", "Number_Of_Deaths")]) ~ .,
  data = df_rpart %>% remove_response_vars(),
  control = rpart.control(
    cp = 0.001,
    maxdepth = 3
  )
)

rpart.plot(rpart_perm, digits=3, faclen=1)

```

The above tree splits on face amount first. Depend


## Intuition: What is rpart doing?



